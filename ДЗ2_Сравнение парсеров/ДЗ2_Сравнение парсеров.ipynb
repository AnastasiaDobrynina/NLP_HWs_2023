{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88d78e36",
   "metadata": {},
   "source": [
    "# ДЗ 2: Сравнение теггеров\n",
    "Анастасия Добрынина, БКЛ-211"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d203a50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c08324f2f3149dabdd887c6339ef460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-07 12:34:59 INFO: Downloading default packages for language: ru (Russian) ...\n",
      "2023-10-07 12:35:00 INFO: File exists: C:\\Users\\79998\\stanza_resources\\ru\\default.zip\n",
      "2023-10-07 12:35:04 INFO: Finished downloading models and saved to C:\\Users\\79998\\stanza_resources.\n",
      "2023-10-07 12:35:04 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ea53bb7b7843e5ba1ea8ffb0a72b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-07 12:35:06 INFO: Loading these models for language: ru (Russian):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | syntagrus |\n",
      "| pos       | syntagrus |\n",
      "| lemma     | syntagrus |\n",
      "| depparse  | syntagrus |\n",
      "| ner       | wikiner   |\n",
      "=========================\n",
      "\n",
      "2023-10-07 12:35:06 INFO: Use device: cpu\n",
      "2023-10-07 12:35:06 INFO: Loading: tokenize\n",
      "2023-10-07 12:35:06 INFO: Loading: pos\n",
      "2023-10-07 12:35:06 INFO: Loading: lemma\n",
      "2023-10-07 12:35:06 INFO: Loading: depparse\n",
      "2023-10-07 12:35:06 INFO: Loading: ner\n",
      "2023-10-07 12:35:07 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk import word_tokenize\n",
    "from string import punctuation\n",
    "punctuation += '—'+'–'\n",
    "\n",
    "import stanza\n",
    "stanza.download('ru')\n",
    "\n",
    "stnz = stanza.Pipeline('ru')\n",
    "\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    Doc\n",
    ")\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "m = Mystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f54c731",
   "metadata": {},
   "source": [
    "### Проблемы автоматической разметки:\n",
    "Рассмотрим случаи, которые вызвают трудности при автоматической разработке. В корпусе будут представлены по 1-2 предложения на каждый пример.\n",
    "- омонимия разного рода: \n",
    "    - существительное vs глагол *печь, ели* \n",
    "    - причастия vs прилагательные *вареный, рассенный*\n",
    "    - наречия vs предлоги: *вблизи*\n",
    "- слова с дефисами. \n",
    "    - Не хотим делить: \n",
    "         - предлоги: *из-за* \n",
    "         - наречия: *по-настоящему*\n",
    "         - названия городов, в том числе другие части речи, образованные от их названий: *Нью-Йорк*, *Йошкар-Олинский*\n",
    "     - Хотим делить:\n",
    "         - частицы: *-таки*\n",
    "         - маршрут: *Белорецк-Магнитогорск*\n",
    "         - диапазон:*5-6*\n",
    "- имена собственные, как экзотические, так и фамилии на *-ов, -их*: *Тарасевича (Тарасевич), Петровский, Лидваль, Шемякин, Резвых, от Гомера до Андре Жида*\n",
    "- сокращения *тыс., ред.*\n",
    "\n",
    "* Категория \"*Редкие слова*\" по статье:\n",
    "    * слова с неизвестным словарю корнем, но образованные с помощью продуктивных аффиксов. В моем корпусе: *симпим (симпить), заспавнилась, коупила (коупить), инфодамплю, галюники, панчи, ремув, на самике (самик), фички*\n",
    "    * совсем непонятные слова. В моем корпусе: *на бэбича (бэбич), упртст, делюлю (прилагательное)*\n",
    "    * окказиональные формы: *стригя, пья, победу, льзя*\n",
    "    * аббривеатуры: *МГУ, ВМО, РФФИ, РАН*\n",
    "\n",
    "Добавлю от себя, что также в соврменных текстах часто встречаются упрощенные формы местоимения, наречий: *покашт, ниче, ченить*. Их тоже хорошо бы размечать, потому что они так же часто будут встречаться в отзывах"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d93bb5c",
   "metadata": {},
   "source": [
    "## Словарь универсальных тегов\n",
    "В рассмотрении будут **Natasha, Stanza, Mystem**\n",
    "\n",
    "Natasha и Stanza используют Upos, у Mystem свои теги. Сравним, как это будет работать для разных частей речи и подберем один враиант"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ecf422",
   "metadata": {},
   "source": [
    "| часть речи | тег Mystem | Upos | Наш итог |\n",
    "|:----------:|:----------:|:---------:|:--------:|\n",
    "| прилагательное | A | ADJ | **ADJ** |\n",
    "| наречие | ADV | ADV | **ADV** |\n",
    "| местоименное наречие | ADVPRO | ADV | **ADV** |\n",
    "| числительное-прилагательное | ANUM | ADJ | **ADJ** |\n",
    "| местоимение-прилагательное | APRO | DET | **DET** |\n",
    "| часть композита - сложного слова | COM | ? | **NOUN** |\n",
    "| сочинительный союз | CONJ | CCONJ | **CONJ** |\n",
    "| подчинительный союз | CONJ | SCONJ | **CONJ** |\n",
    "| междометие | INTJ | INTJ | **INTJ** |\n",
    "| числительное | NUM | NUM | **NUM** |\n",
    "| частица | PART | PART | **PART** |\n",
    "| предлог| PR | ADP | **ADP** |\n",
    "| существительное| S | NOUN | **NOUN** |\n",
    "| местоимение-существительное| SPRO | PRON | **PRON** |\n",
    "| глагол | V | VERB | **VERB** |\n",
    "| вспомогательный глагол | V | AUX | **VERB**\n",
    "| имя собственное | S | PROPN | **NOUN** |\n",
    "| пунктуация | - | PUNCT | **NA** |\n",
    "| символ | - | SYM | **NA** |\n",
    "\n",
    "\n",
    "\n",
    "Итог: в основном теги в Stanza и Natasha останутся нетронутыми. Имена собственные вспомогательные глаголы и союзы будут более обощены, а символы и пункутуация вовсе не будут учитываться.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46085b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_frame = pd.DataFrame(columns = ['Токен', 'Тег_1', 'Тег_2']) # второй тег для неоднозначных по моему мнению случаев"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc77f64",
   "metadata": {},
   "source": [
    "Использую токенизацию с помощью nltk для удобства и превращаю корпус в таблицу. Далее надо разметить вручную все токены, проверить правильность токенизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43646a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpora.txt', 'r', encoding='utf-8') as file:\n",
    "    raw_text = file.read()\n",
    "    tokens_text = word_tokenize(raw_text)\n",
    "    tokens_frame['Токен'] = tokens_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58a879d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_frame.to_csv('empty_tokens.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "298e8825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# подгружаю размеченные вручную токены\n",
    "gold_taggs = pd.read_csv('tagged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ee88e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Токен</th>\n",
       "      <th>Тег_1</th>\n",
       "      <th>Тег_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Входим</td>\n",
       "      <td>VERB</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>в</td>\n",
       "      <td>ADP</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>дом</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>,</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>открыв</td>\n",
       "      <td>VERB</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Токен Тег_1 Тег_2\n",
       "0  Входим  VERB   NaN\n",
       "1       в   ADP   NaN\n",
       "2     дом  NOUN   NaN\n",
       "3       ,   NaN   NaN\n",
       "4  открыв  VERB   NaN"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_taggs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35b84a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер корпуса:  625\n"
     ]
    }
   ],
   "source": [
    "print('Размер корпуса: ', len(gold_taggs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76568aa0",
   "metadata": {},
   "source": [
    "## Задаем функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5566d99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "upos_tags = {'CCONJ': 'CONJ',\n",
    "             'SCONJ': 'CONJ',\n",
    "             'AUX': 'VERB',\n",
    "             'PROPN': 'NOUN',\n",
    "             'PUNCT': '',\n",
    "             'SYM': ''\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97fa4279",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem_tags = {'A': 'ADJ', \n",
    "               'ADVPRO': 'ADV', \n",
    "               'ANUM': 'ADJ', \n",
    "               'APRO': 'DET', \n",
    "               'COM': 'NOUN', \n",
    "               'PR': 'ADP', \n",
    "               'S': 'NOUN',\n",
    "               'SPRO': 'PRON',\n",
    "               'V': 'VERB',\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8426bfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_right_tokens(token, tag, gold, results, gold_tags, index):\n",
    "    token_gold = gold['Токен'][index]\n",
    "    tag_gold_1 = gold['Тег_1'][index]\n",
    "    tag_gold_2 = gold['Тег_2'][index]\n",
    "    \n",
    "    if tag == tag_gold_1:\n",
    "        gold_tags.append(tag_gold_1)\n",
    "    elif tag == tag_gold_2:\n",
    "         gold_tags.append(tag_gold_2)\n",
    "    else: gold_tags.append(tag_gold_1)\n",
    "    results.append(tag)\n",
    "    index += 1\n",
    "    return index, results, gold_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59f35020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef compare(gold, unified_parser):\\n    results = []  # сюда будем писать результаты\\n    gold_tags = []     # сюда будем писать мои теги\\n    index = 0\\n    for token, tag, in zip(unified_parser['Токен'], unified_parser['Тег']):\\n        token_gold = gold['Токен'][index]\\n        tag_gold_1 = gold['Тег_1'][index]\\n        tag_gold_2 = gold['Тег_2'][index]\\n                                 \\n        # если все нормально с токинизацией\\n        if token == token_gold:\\n            res = compare_right_tokens(token, tag, gold, results, gold_tags, index)\\n            index = res[0] \\n            results = res[1]\\n            gold_tags = res[2]\\n                                 \\n        # если неправильная токенизация\\n        else: \\n            if token in token_gold: # парсер побил на токены лишний раз\\n                print('Парсер перебил ' + str(token_gold) + '. Выдал: ' + str(token))\\n                \\n                if token_gold.endswith(token):\\n                    results.append('')\\n                    gold_tags.append(tag_gold_1)\\n                    index += 1\\n                else: continue # без прибавления счетчика, пока не дойдем до конца токена\\n            \\n            elif token_gold in token: # парсер недобил \\n                print('Парсер недобил ' + str(token_gold) + '. Выдал: ' + str(token))\\n                while not token.endswith(token_gold): \\n                    token_gold = gold['Токен'][index]\\n                    index += 1\\n                results.append('')\\n                gold_tags.append(gold['Тег_1'][index])\\n                \\n            else: # mystem пропускает некоторые токены, например, знаки препинания\\n                index += 1\\n                res = compare_right_tokens(token, tag, gold, results, gold_tags, index)\\n                index = res[0] \\n                results = res[1]\\n                gold_tags = res[2]\\n                \\n    return accuracy_score(results, gold_tags)\\n    \""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def compare(gold, unified_parser):\n",
    "    results = []  # сюда будем писать результаты\n",
    "    gold_tags = []     # сюда будем писать мои теги\n",
    "    index = 0\n",
    "    for token, tag, in zip(unified_parser['Токен'], unified_parser['Тег']):\n",
    "        token_gold = gold['Токен'][index]\n",
    "        tag_gold_1 = gold['Тег_1'][index]\n",
    "        tag_gold_2 = gold['Тег_2'][index]\n",
    "                                 \n",
    "        # если все нормально с токинизацией\n",
    "        if token == token_gold:\n",
    "            res = compare_right_tokens(token, tag, gold, results, gold_tags, index)\n",
    "            index = res[0] \n",
    "            results = res[1]\n",
    "            gold_tags = res[2]\n",
    "                                 \n",
    "        # если неправильная токенизация\n",
    "        else: \n",
    "            if token in token_gold: # парсер побил на токены лишний раз\n",
    "                print('Парсер перебил ' + str(token_gold) + '. Выдал: ' + str(token))\n",
    "                \n",
    "                if token_gold.endswith(token):\n",
    "                    results.append('')\n",
    "                    gold_tags.append(tag_gold_1)\n",
    "                    index += 1\n",
    "                else: continue # без прибавления счетчика, пока не дойдем до конца токена\n",
    "            \n",
    "            elif token_gold in token: # парсер недобил \n",
    "                print('Парсер недобил ' + str(token_gold) + '. Выдал: ' + str(token))\n",
    "                while not token.endswith(token_gold): \n",
    "                    token_gold = gold['Токен'][index]\n",
    "                    index += 1\n",
    "                results.append('')\n",
    "                gold_tags.append(gold['Тег_1'][index])\n",
    "                \n",
    "            else: # mystem пропускает некоторые токены, например, знаки препинания\n",
    "                index += 1\n",
    "                res = compare_right_tokens(token, tag, gold, results, gold_tags, index)\n",
    "                index = res[0] \n",
    "                results = res[1]\n",
    "                gold_tags = res[2]\n",
    "                \n",
    "    return accuracy_score(results, gold_tags)\n",
    "    '''\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f618490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(gold, unified_parser):\n",
    "    results = []  # сюда будем писать результаты\n",
    "    gold_tags = []     # сюда будем писать мои теги\n",
    "    index = 0\n",
    "    for token, tag, in zip(unified_parser['Токен'], unified_parser['Тег']):\n",
    "        token_gold = gold['Токен'][index]\n",
    "        tag_gold_1 = gold['Тег_1'][index]\n",
    "        tag_gold_2 = gold['Тег_2'][index]\n",
    "                                 \n",
    "        # если все нормально с токинизацией\n",
    "        if token == token_gold:\n",
    "            res = compare_right_tokens(token, tag, gold, results, gold_tags, index)\n",
    "            index = res[0] \n",
    "            results = res[1]\n",
    "            gold_tags = res[2]\n",
    "                                 \n",
    "        # если неправильная токенизация\n",
    "        else: \n",
    "            if token in str(token_gold): # парсер побил на токены лишний раз\n",
    "                print('Парсер перебил ' + str(token_gold) + '. Выдал: ' + str(token))\n",
    "                \n",
    "                if token_gold.endswith(token):\n",
    "                    results.append('')\n",
    "                    gold_tags.append(tag_gold_1)\n",
    "                    index += 1\n",
    "                else: continue # без прибавления счетчика, пока не дойдем до конца токена\n",
    "            \n",
    "            elif str(token_gold) in token: # парсер недобил \n",
    "                print('Парсер недобил ' + str(token_gold) + '. Выдал: ' + str(token))\n",
    "                while not token.endswith(token_gold): \n",
    "                    token_gold = gold['Токен'][index]\n",
    "                    index += 1\n",
    "                results.append('')\n",
    "                gold_tags.append(gold['Тег_1'][index])\n",
    "                \n",
    "            else: \n",
    "                print('Не совпадают:', token_gold, token)\n",
    "                \n",
    "    return accuracy_score(results, gold_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b935d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем таблицу с разметкой теггера\n",
    "def create_tagged_frame(tag_dict):\n",
    "    new_frame = pd.DataFrame(columns = ['Токен', 'Тег'], index = range(0, len(tag_dict)))\n",
    "    tokens = [token[0] for token in tag_dict]\n",
    "    tags = [tag[1] for tag in tag_dict]\n",
    "    new_frame['Токен'] = tokens\n",
    "    new_frame['Тег'] = tags\n",
    "    return new_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d639fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unification(dct):\n",
    "    unified = []\n",
    "    for token_n_tag in dct:\n",
    "        token = token_n_tag[0]\n",
    "        tag = token_n_tag[1]\n",
    "        if tag in mystem_tags:\n",
    "            unified.append((token, mystem_tags[tag]))\n",
    "        elif tag in upos_tags:\n",
    "            unified.append((token, upos_tags[tag]))\n",
    "        else: unified.append((token, tag))\n",
    "    return create_tagged_frame(unified)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00297e99",
   "metadata": {},
   "source": [
    "# Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7be9d5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min\n",
      "Wall time: 11.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "stanza_dict = [] # на самом деле не слвоарь, а лист коретежей, чтобы значения могли повторяться\n",
    "analys = stnz(raw_text).to_dict()\n",
    "for i in range(0, len(analys)):\n",
    "    for sent in analys[i]:\n",
    "        stanza_dict.append((sent['text'], sent['upos']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3a4776f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Входим', 'VERB'),\n",
       " ('в', 'ADP'),\n",
       " ('дом', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('открыв', 'VERB'),\n",
       " ('дверь', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('попадаем', 'VERB'),\n",
       " ('в', 'ADP'),\n",
       " ('кухню', 'NOUN')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stanza_dict[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0dbb5b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza_frame = unification(stanza_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "700c469e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Токен</th>\n",
       "      <th>Тег</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Входим</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>в</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>дом</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>открыв</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Токен   Тег\n",
       "0  Входим  VERB\n",
       "1       в   ADP\n",
       "2     дом  NOUN\n",
       "3       ,      \n",
       "4  открыв  VERB"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stanza_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "befeeaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Парсер недобил Немцы. Выдал: Немцы-таки\n",
      "Парсер недобил Белорецк. Выдал: Белорецк-Магнитогорск\n",
      "Парсер перебил Йошкар-Олинском. Выдал: Йошкар-\n",
      "Парсер перебил Йошкар-Олинском. Выдал: Олинском\n",
      "Парсер недобил Прим. Выдал: Прим.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7607260726072608"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(gold_taggs, stanza_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7a88d0",
   "metadata": {},
   "source": [
    "# Natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a25f3152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 125 ms\n",
      "Wall time: 82.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "natasha_dict = [] # на самом деле не слвоарь, а лист коретежей, чтобы значения могли повторяться\n",
    "doc = Doc(raw_text)\n",
    "doc.segment(segmenter)\n",
    "doc.tag_morph(morph_tagger)\n",
    "for token in doc.tokens:\n",
    "    natasha_dict.append((token.text, token.pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d62ad69d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Входим', 'VERB'),\n",
       " ('в', 'ADP'),\n",
       " ('дом', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('открыв', 'VERB'),\n",
       " ('дверь', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('попадаем', 'VERB'),\n",
       " ('в', 'ADP'),\n",
       " ('кухню', 'NOUN')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "natasha_dict[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e945f894",
   "metadata": {},
   "outputs": [],
   "source": [
    "natasha_frame = unification(natasha_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdf06b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Токен</th>\n",
       "      <th>Тег</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Входим</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>в</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>дом</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>открыв</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Токен   Тег\n",
       "0  Входим  VERB\n",
       "1       в   ADP\n",
       "2     дом  NOUN\n",
       "3       ,      \n",
       "4  открыв  VERB"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "natasha_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f79be148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Парсер недобил 5. Выдал: 5-6\n",
      "Парсер недобил Немцы. Выдал: Немцы-таки\n",
      "Парсер перебил А.. Выдал: А\n",
      "Парсер перебил А.. Выдал: .\n",
      "Парсер перебил Е.. Выдал: Е\n",
      "Парсер перебил Е.. Выдал: .\n",
      "Парсер недобил Белорецк. Выдал: Белорецк-Магнитогорск\n",
      "Парсер недобил 2001. Выдал: 2001-2002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7479270315091211"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(gold_taggs, natasha_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643f3a25",
   "metadata": {},
   "source": [
    "# Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5eb1fbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 38.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc = m.analyze(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f56fb60",
   "metadata": {},
   "source": [
    "Заметим, что работает в разы дольше соперниц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f0bcb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem_dict = [] # на самом деле не слвоарь, а лист коретежей, чтобы значения могли повторяться\n",
    "for i in range(0, len(doc)):\n",
    "    txt = doc[i]['text'].replace(' ', '') #приклеивает пробелы к пунктуации\n",
    "    try:\n",
    "        mystem_dict.append((txt, doc[i]['analysis'][0]['gr'].split('=')[0].split(',')[0]))\n",
    "    except: \n",
    "        if txt.isalpha() or txt.isdigit(): #пропускает незнакомые слова, числа и пунктуацию, что плохо\n",
    "            mystem_dict.append((txt, ' '))\n",
    "        else: \n",
    "            if (txt in punctuation or txt[0] in punctuation) and txt != '':\n",
    "                mystem_dict.append((txt, ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f8130bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Входим', 'V'),\n",
       " ('в', 'PR'),\n",
       " ('дом', 'S'),\n",
       " (',', ' '),\n",
       " ('открыв', 'V'),\n",
       " ('дверь', 'S'),\n",
       " (',', ' '),\n",
       " ('попадаем', 'V'),\n",
       " ('в', 'PR'),\n",
       " ('кухню', 'S')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystem_dict[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "697f0ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem_frame = unification(mystem_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d47d1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Парсер перебил А.. Выдал: А\n",
      "Парсер перебил А.. Выдал: .\n",
      "Парсер перебил Е.. Выдал: Е\n",
      "Парсер перебил Е.. Выдал: .\n",
      "Парсер перебил 15,5. Выдал: 15\n",
      "Парсер перебил 15,5. Выдал: ,\n",
      "Парсер перебил 15,5. Выдал: 5\n",
      "Парсер недобил .. Выдал: .):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7454844006568144"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(gold_taggs, mystem_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f57e24",
   "metadata": {},
   "source": [
    "### Итог\n",
    "Лучшая точность у **Stanza**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83b5dcb",
   "metadata": {},
   "source": [
    "# Применяем к анализу тональности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2837f3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reviews.json') as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00de3f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_reviews.json') as json_file:\n",
    "    test_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4003bd",
   "metadata": {},
   "source": [
    "## Какие паттерны берем?\n",
    "\n",
    "* **не + ADJ** и **не + ADV**. Могут быть сочетания типа *не плохо работает* или *далеко не лучший кинтеатр*\n",
    "\n",
    "* **ADV + ADJ**. Интесификаторы, типа *невероятно ужасный*, могут быть одними и теми же в отзывах обоих тональностей, хотя сами по себе несут положительную семантику: *Сайт работает невероятно!*\n",
    "\n",
    "* **ADJ + \"качество\"** и **ADV + \"реклама\"**. Судя по словарю из предыдущей домашки, слова *качество* и *реклама* довольно частотны в словаре до удаления повторов. Есть смысл посмотреть на его сочетания с разными прилагательными (*отличное качетсво, ужасное качество*) и наречиями (*мало рекламы*, *много рекламы*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abde2e1",
   "metadata": {},
   "source": [
    "Освежим в памяти прошлый код, но теперь лемматизация будет с помощью Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "beb97a42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess_and_count(data):\n",
    "    lemmas_count = Counter()\n",
    "    for text in data:\n",
    "        doc = stnz(text).to_dict()\n",
    "        for sent in doc:\n",
    "            for word in sent:\n",
    "                lemma = word['lemma']\n",
    "                if lemma.isalpha():\n",
    "                    lemmas_count.update([lemma])\n",
    "    return(lemmas_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea8cc64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 13min 58s\n",
      "Wall time: 2min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "good_lemmas_count = preprocess_and_count(data['good'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd01e5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 12min 28s\n",
      "Wall time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bad_lemmas_count = preprocess_and_count(data['bad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b980e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего хороших лемм: 1687\n",
      "Всего плохих лемм: 1678\n"
     ]
    }
   ],
   "source": [
    "print('Всего хороших лемм:', len(good_lemmas_count))\n",
    "print('Всего плохих лемм:', len(bad_lemmas_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9dd8586a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate (gl, bl):\n",
    "    good_lemmas_filtered = Counter()\n",
    "\n",
    "    new_good_lemmas = {key: value for key, value in dict(gl).items()}\n",
    "    new_bad_lemmas = {key: value for key, value in dict(bl).items()}\n",
    "    \n",
    "    for key, value in new_good_lemmas.items():\n",
    "        if key in new_bad_lemmas:\n",
    "            del new_bad_lemmas[key]\n",
    "        else:\n",
    "            good_lemmas_filtered[key] = value\n",
    "\n",
    "    bad_lemmas_filtered = Counter(new_bad_lemmas)\n",
    "    return (good_lemmas_filtered, bad_lemmas_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b2083f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_list(x, good, bad):\n",
    "    res = deduplicate(good, bad)\n",
    "    good_dict = res[0]\n",
    "    bad_dict = res[1]\n",
    "    freq_list = {'good': {}, 'bad': {}}\n",
    "    for word, score in good_dict.items():\n",
    "        if score > x:\n",
    "            freq_list['good'][word] = score\n",
    "    for word, score in bad_dict.items():\n",
    "        if score > x:\n",
    "            freq_list['bad'][word] = score\n",
    "    return freq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "49f91b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_list_simple_1 = get_freq_list(1, good_lemmas_count, bad_lemmas_count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a10ff987",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_list_simple_0 = get_freq_list(0, good_lemmas_count, bad_lemmas_count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1c5f892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(freq_lists, text):\n",
    "    counts = Counter({'good': 0, 'bad': 0})\n",
    "    for word in text.split():\n",
    "        if word in freq_lists['good']:\n",
    "            counts['good'] += int(freq_lists['good'][word])\n",
    "        elif word in freq_lists['bad']:\n",
    "            counts['bad'] += int(freq_lists['bad'][word])\n",
    "    return counts.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ec648f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция, которая только токенизирует слова, приводит к нижнему регистру, начальной форме\n",
    "def preprocess(texts):\n",
    "    lemmas = set()\n",
    "    for text in texts:\n",
    "        lemmas_txt = ''\n",
    "        doc = stnz(text).to_dict()\n",
    "        for sent in doc:\n",
    "            for word in sent:\n",
    "                lemma = word['lemma']\n",
    "                if lemma.isalpha():\n",
    "                    lemmas_txt += lemma + \" \"\n",
    "        lemmas.add(lemmas_txt)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c8f4c0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_detect(good_test, bad_test, freq_lists):\n",
    "    results = []  # сюда будем писать результаты\n",
    "    gold = []     # сюда будем писать исходную тональность\n",
    "    for review in good_test:\n",
    "        predicted_mood = detect(freq_lists, review)\n",
    "        results.append(predicted_mood)\n",
    "        gold.append('good')\n",
    "    for review in bad_test:\n",
    "        predicted_mood = detect(freq_lists, review)\n",
    "        results.append(predicted_mood)\n",
    "        gold.append('bad')\n",
    "    return accuracy_score(results, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "354c4516",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_good = preprocess(test_data['good'])\n",
    "test_bad = preprocess(test_data['bad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5b25f576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_detect(test_good, test_bad, freq_list_simple_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f3c59898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_detect(test_good, test_bad, freq_list_simple_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b4ba86",
   "metadata": {},
   "source": [
    "И вот теперь..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3b163de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_all = ['ADV ADJ', 'не ADJ', 'не ADV', 'ADJ качество', 'ADV реклама', 'качество ADJ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e4befd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# подсчет частотности + чанки\n",
    "def preprocess_and_count_chunks(data, chunks):\n",
    "    previous_pos = ''\n",
    "    previous_lemma = ''\n",
    "    \n",
    "    lemmas_count = Counter()\n",
    "    for text in data:\n",
    "        doc = stnz(text).to_dict()\n",
    "        for sent in doc:\n",
    "            for word in sent:\n",
    "                lemma = word['lemma']\n",
    "                pos = word['upos']\n",
    "                if lemma.isalpha():\n",
    "                    if (previous_pos + ' ' + pos) in chunks:\n",
    "                        lemmas_count.update([str(previous_lemma + ' ' + lemma)])\n",
    "                        print(previous_lemma + ' ' + lemma)\n",
    "                        \n",
    "                    elif (previous_pos + ' ' + lemma) in chunks:\n",
    "                        lemmas_count.update([str(previous_lemma + ' ' + lemma)])\n",
    "                        print(previous_lemma + ' ' + lemma)\n",
    "                        \n",
    "                    elif (previous_lemma + ' ' + pos) in chunks:\n",
    "                        lemmas_count.update([str(previous_lemma + ' ' + lemma)])\n",
    "                        print(previous_lemma + ' ' + lemma)\n",
    "                    else: lemmas_count.update([lemma])\n",
    "                previous_pos = pos\n",
    "                previous_lemma = lemma\n",
    "                    \n",
    "    return lemmas_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8849795a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "примерно одинаковый\n",
      "лучший качество\n",
      "наилучший качество\n",
      "очень супирй\n",
      "отличный качество\n",
      "вполне достаточный\n",
      "не полностью\n",
      "не интересный\n",
      "много хороший\n",
      "не нужный\n",
      "более удобный\n",
      "не менее\n",
      "не маленький\n",
      "уже хороший\n",
      "не удобно\n",
      "довольно интересный\n",
      "не очень\n",
      "очень опытный\n",
      "много хороший\n",
      "очень удобный\n",
      "много интересный\n",
      "отличный качество\n",
      "очень дорогой\n",
      "особенно советский\n",
      "очень удивленный\n",
      "не так\n",
      "очень маленький\n",
      "так доступный\n",
      "не надо\n",
      "отличный качество\n",
      "очень хороший\n",
      "очень интересный\n",
      "очень отличный\n",
      "не удобный\n",
      "очень удобный\n",
      "качество отличный\n",
      "интуитивно понятный\n",
      "много интересный\n",
      "не очень\n",
      "очень удобный\n",
      "достаточно старый\n",
      "очень странный\n",
      "много платный\n",
      "много платный\n",
      "не менее\n",
      "очень хороший\n",
      "очень удобный\n",
      "очень удобный\n",
      "более длительный\n",
      "сейчас разнообразный\n",
      "очень распространенный\n",
      "не так\n",
      "достаточно большой\n",
      "серийно очередной\n",
      "несколько затруднительный\n",
      "не качественный\n",
      "высокий качество\n",
      "очень довольный\n",
      "довольный качество\n",
      "интуитивно понятный\n",
      "максимально простой\n",
      "действительно самый\n",
      "приятно удивленный\n",
      "высокий качество\n",
      "качество основной\n",
      "удобно расположенный\n",
      "4к качество\n",
      "очень удобный\n",
      "хороший качество\n",
      "хороший качество\n",
      "не навязчивый\n",
      "вообщем отличный\n",
      "где понятный\n",
      "хороший качество\n",
      "очень удобный\n",
      "очень удобный\n",
      "там большой\n",
      "не огромный\n",
      "очень удобный\n",
      "довольно подробный\n",
      "довольно выгодный\n",
      "достаточно простой\n",
      "бесплатно пробный\n",
      "давно знакомый\n",
      "уже сложный\n",
      "точно красочно\n",
      "дейтствительно хороший\n",
      "очень простой\n",
      "не всегда\n",
      "всегда корректный\n",
      "более богатый\n",
      "отличный качество\n",
      "очень удобный\n",
      "очень крутой\n",
      "конечно английский\n",
      "еще другой\n",
      "уже побочный\n",
      "крайне недовольный\n",
      "не так\n",
      "не прав\n",
      "очень популярный\n",
      "не хороший\n",
      "не плохой\n",
      "не обязательный\n",
      "бесплатно возможный\n",
      "не китайский\n",
      "вполне возможный\n",
      "не самый\n",
      "там удобный\n",
      "не так\n",
      "уже довольный\n",
      "не очень\n",
      "очень удобный\n",
      "как-то мутный\n",
      "не очень\n",
      "очень понятный\n",
      "низкий качество\n",
      "высокий качество\n",
      "как-то простой\n",
      "потрясающий качество\n",
      "очень удобный\n",
      "прекрасно читаемый\n",
      "очень удобный\n",
      "очень удобный\n",
      "очень довольный\n",
      "не впервые\n",
      "не должен\n",
      "тупо красивый\n",
      "не самый\n",
      "не правда\n",
      "поистине обширный\n",
      "очень известный\n",
      "наилучший качество\n",
      "больно дорогой\n",
      "очень удобный\n",
      "очень странный\n",
      "очень удобный\n",
      "интуитивно понятный\n",
      "CPU times: total: 12min 12s\n",
      "Wall time: 2min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "good_chunks_count = preprocess_and_count_chunks(data['good'], chunks_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b7ddbe40",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "не удобный\n",
      "не адекватный\n",
      "много реклама\n",
      "не так\n",
      "не доступный\n",
      "хороший качество\n",
      "достаточно скудный\n",
      "более приятный\n",
      "не кликабельный\n",
      "более доступный\n",
      "очень неудобный\n",
      "очень небольшой\n",
      "очень старый\n",
      "очень странный\n",
      "не надо\n",
      "не густой\n",
      "не руский\n",
      "не лестный\n",
      "хороший качество\n",
      "не менее\n",
      "менее дорогой\n",
      "очень довольный\n",
      "удачно целый\n",
      "желательно бесплатный\n",
      "не должен\n",
      "где нужный\n",
      "уже достаточный\n",
      "там сплошной\n",
      "однако уверенный\n",
      "не технический\n",
      "столько сложный\n",
      "более удобный\n",
      "сейчас нужный\n",
      "столько лишний\n",
      "не нужный\n",
      "абсолютно невнятный\n",
      "абсолютно бесполезный\n",
      "совсем пыльный\n",
      "хороший качество\n",
      "очень отстойный\n",
      "не обязательный\n",
      "не самый\n",
      "не приятный\n",
      "срочно блокируйтый\n",
      "совершенно ужасный\n",
      "не так\n",
      "не тогда\n",
      "можно разный\n",
      "так приятный\n",
      "невероятный обширный\n",
      "не голливудский\n",
      "настолько значительный\n",
      "крайне редкий\n",
      "не оттуда\n",
      "не возможный\n",
      "много хороший\n",
      "очень старый\n",
      "очень осторожный\n",
      "не приятный\n",
      "не правда\n",
      "качество пробный\n",
      "внезапно невозможный\n",
      "не удобный\n",
      "крайне осторожный\n",
      "не даром\n",
      "лучший качество\n",
      "не удобный\n",
      "не удобный\n",
      "естественно второй\n",
      "не иначе\n",
      "абсолютно незнакомый\n",
      "CPU times: total: 11min 54s\n",
      "Wall time: 2min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bad_chunks_count = preprocess_and_count_chunks(data['bad'], chunks_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cd818d",
   "metadata": {},
   "source": [
    "Собранные биграммы выглядят осмысленно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "50d3c6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# по предыдущему оптыу, без фильтрации по частотности выходит лучше\n",
    "freq_list_chunks = get_freq_list(0, good_chunks_count, bad_chunks_count) \n",
    "test_detect(test_good, test_bad, freq_list_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fbebe8",
   "metadata": {},
   "source": [
    "Качество не улучшилось, но возможно на большем количестве материала выделение чанков даст результат"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
